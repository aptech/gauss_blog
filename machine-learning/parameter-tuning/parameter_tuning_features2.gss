new;
library gml;
rndseed 23423;

/*
** Load data and prepare data
*/
// Load dataset
dataset = __FILE_DIR $+ "reg_data.gdat";
data = loadd(dataset);

/*
** Extract outcome and features
*/
// Extract outcome variable
y = data[., "CBO_GAP"];

// Extract features
X = delcols(data, "date"$|"CBO_GAP");

/*
** Split data into 70% training
** and 30% testing sets
*/
shuffle = "False";
{ y_train, y_test, x_train, x_test } = traintestSplit(Y, X, 0.7, shuffle);

/*
** Settings for decision forest
*/
// Declare an instance of the
// dfControl structure
struct dfControl dfc;

// Set default values for
// structure members
dfc = dfControlCreate();

// Turn on out-of-bag error calculation
dfc.oobError = 1;

// Set features per node
dfc.featuresPerNode = 6;

/*
** Initialize grid and
** storage matrices
*/
// Set potential values for
// minimum samples per leaf
minimumSamplesLeaf = seqa(1, 1, 20);

// Set potential values for
// percentage of sample
// per tree
pctSample = seqa(0.1, 0.1, 10);

// Storage matrices
test_mse = zeros(rows(minimumSamplesLeaf), rows(pctSample));
train_mse = zeros(rows(minimumSamplesLeaf), rows(pctSample));

for j(1, rows(minimumSamplesLeaf), 1);
    
    // Set the minimum samples per leaf
    dfc.minObsNode = minimumSamplesLeaf[j];
    
    for i(1, rows(pctSample), 1);
        
        // Set percentage of sample per tree
        dfc.obsPerTree = pctSample[i];
        
        /*
        ** Decision Forest Model
        */
        // Declare 'mdl' to be an instance of a
        // dfModel structure to hold the estimation results
        struct dfModel mdl;
        
        // Estimate the model with default settings
        mdl = decForestRFit(y_train, x_train, dfc);
        
        // Make predictions using training data
        df_prediction_train = decForestPredict(mdl, x_train);
        
        // Make predictions using testing data
        df_prediction_test = decForestPredict(mdl, x_test);
        
        /*
        ** Compute and store mse
        */
        // Training set MSE
        train_mse[j, i] = meanSquaredError(y_train, df_prediction_train);
        
        // Testing set MSE
        test_mse[j, i] = meanSquaredError(y_test, df_prediction_test);
  
    endfor;
endfor;

/*
** Locate minimum for each 
** for each value of dfc.obsPerTree
** and associated dfc.minObsNode
*/
// Find row indices of minimum MSE
// for each potential value of dfc.obsPerTree
indx1 = minindc(test_mse);

// Find values of minimum MSE for
// each potential value of dfc.obsPerTree
tmp = minc(test_mse);

/*
** Locate optimal MSE
*/
indx2 = minindc(tmp);
MSE_optimal = minc(tmp);

sprintf("Minimum MSE occurs at a minimum samples per leaf of %d  
combined with a percentage of samples per tree of %1.0f%%.", 
        minimumSamplesLeaf[indx1[indx2]], 100*pctSample[indx2]);
sprintf( "Minimum testing MSE %4f.", MSE_optimal);
